package vibecoding

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"path/filepath"
	"sort"
	"strings"
	"time"

	"ai-chatter/internal/llm"
)

// ProjectContext –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è LLM
type ProjectContext struct {
	ProjectName  string           `json:"project_name"`
	Language     string           `json:"language"`
	GeneratedAt  time.Time        `json:"generated_at"`
	TotalFiles   int              `json:"total_files"`
	Description  string           `json:"description"`
	Dependencies []string         `json:"dependencies,omitempty"`
	Files        []FileSignature  `json:"files"`
	Structure    ProjectStructure `json:"structure"`
}

// FileSignature —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É —Ñ–∞–π–ª–∞ (—Ñ—É–Ω–∫—Ü–∏–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã)
type FileSignature struct {
	Path       string      `json:"path"`
	Type       string      `json:"type"` // "go", "js", "py", "md", "json", etc.
	Size       int         `json:"size"`
	Functions  []Function  `json:"functions,omitempty"`
	Structs    []Struct    `json:"structs,omitempty"`
	Interfaces []Interface `json:"interfaces,omitempty"`
	Constants  []Variable  `json:"constants,omitempty"`
	Variables  []Variable  `json:"variables,omitempty"`
	Imports    []string    `json:"imports,omitempty"`
	Summary    string      `json:"summary,omitempty"`
}

// Function –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É —Ñ—É–Ω–∫—Ü–∏–∏
type Function struct {
	Name       string   `json:"name"`
	Signature  string   `json:"signature"`
	Receiver   string   `json:"receiver,omitempty"`
	Parameters []string `json:"parameters,omitempty"`
	Returns    []string `json:"returns,omitempty"`
	IsExported bool     `json:"is_exported"`
	Comment    string   `json:"comment,omitempty"`
	LineNumber int      `json:"line_number"`
}

// Struct –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
type Struct struct {
	Name       string  `json:"name"`
	Fields     []Field `json:"fields,omitempty"`
	IsExported bool    `json:"is_exported"`
	Comment    string  `json:"comment,omitempty"`
	LineNumber int     `json:"line_number"`
}

// Interface –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
type Interface struct {
	Name       string   `json:"name"`
	Methods    []Method `json:"methods,omitempty"`
	IsExported bool     `json:"is_exported"`
	Comment    string   `json:"comment,omitempty"`
	LineNumber int      `json:"line_number"`
}

// Field –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
type Field struct {
	Name string `json:"name"`
	Type string `json:"type"`
	Tag  string `json:"tag,omitempty"`
}

// Method –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
type Method struct {
	Name       string   `json:"name"`
	Signature  string   `json:"signature"`
	Parameters []string `json:"parameters,omitempty"`
	Returns    []string `json:"returns,omitempty"`
}

// Variable –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
type Variable struct {
	Name       string `json:"name"`
	Type       string `json:"type"`
	Value      string `json:"value,omitempty"`
	IsExported bool   `json:"is_exported"`
	Comment    string `json:"comment,omitempty"`
}

// ProjectStructure –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
type ProjectStructure struct {
	Directories []Directory `json:"directories"`
	FileTypes   []FileType  `json:"file_types"`
}

// Directory –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
type Directory struct {
	Path      string `json:"path"`
	FileCount int    `json:"file_count"`
	Purpose   string `json:"purpose,omitempty"`
}

// FileType –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ —Ñ–∞–π–ª–æ–≤
type FileType struct {
	Extension string `json:"extension"`
	Count     int    `json:"count"`
	Language  string `json:"language"`
}

// LLMContextGenerator –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
type LLMContextGenerator struct {
	llmClient      llm.Client
	maxTokens      int // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
	tokenEstimator *TokenEstimator
}

// TokenEstimator –ø—Ä–∏–º–µ—Ä–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
type TokenEstimator struct{}

// ProjectContextLLM –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLM-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
type ProjectContextLLM struct {
	ProjectName  string                    `json:"project_name"`
	Language     string                    `json:"language"`
	GeneratedAt  time.Time                 `json:"generated_at"`
	TotalFiles   int                       `json:"total_files"`
	Description  string                    `json:"description"`
	Dependencies []string                  `json:"dependencies,omitempty"`
	Files        map[string]LLMFileContext `json:"files"` // path -> context
	Structure    ProjectStructure          `json:"structure"`
	TokensUsed   int                       `json:"tokens_used"`
	TokensLimit  int                       `json:"tokens_limit"`
}

// LLMFileContext —Å–æ–¥–µ—Ä–∂–∏—Ç LLM-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
type LLMFileContext struct {
	Path         string    `json:"path"`
	Type         string    `json:"type"`
	Size         int       `json:"size"`
	LastModified time.Time `json:"last_modified"`
	Summary      string    `json:"summary"`      // LLM-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
	KeyElements  []string  `json:"key_elements"` // –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏/—Å—Ç—Ä—É–∫—Ç—É—Ä—ã/–∫–ª–∞—Å—Å—ã
	Purpose      string    `json:"purpose"`      // –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
	Dependencies []string  `json:"dependencies"` // –§–∞–π–ª—ã, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–≤–∏—Å–∏—Ç
	TokensUsed   int       `json:"tokens_used"`  // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
	NeedsUpdate  bool      `json:"needs_update"` // –§–ª–∞–≥ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
}

// ContextGenerationRequest –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ–ø–∏—Å–∞–Ω–∏—è —Ñ–∞–π–ª–∞
type ContextGenerationRequest struct {
	ProjectName     string            `json:"project_name"`
	Language        string            `json:"language"`
	FilePath        string            `json:"file_path"`
	FileContent     string            `json:"file_content,omitempty"`
	FileList        []string          `json:"file_list"`        // –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞
	ExistingContext map[string]string `json:"existing_context"` // –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤
	TokenBudget     int               `json:"token_budget"`     // –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
}

// ContextGenerationResponse –æ—Ç–≤–µ—Ç LLM —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞
type ContextGenerationResponse struct {
	Summary      string   `json:"summary"`
	KeyElements  []string `json:"key_elements"`
	Purpose      string   `json:"purpose"`
	Dependencies []string `json:"dependencies"`
	Language     string   `json:"language,omitempty"`
}

// NewLLMContextGenerator —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π LLM-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
func NewLLMContextGenerator(llmClient llm.Client, maxTokens int) *LLMContextGenerator {
	if maxTokens <= 0 {
		maxTokens = 5000 // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 5000 —Ç–æ–∫–µ–Ω–æ–≤
	}

	return &LLMContextGenerator{
		llmClient:      llmClient,
		maxTokens:      maxTokens,
		tokenEstimator: &TokenEstimator{},
	}
}

// EstimateTokens –ø—Ä–∏–º–µ—Ä–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
func (te *TokenEstimator) EstimateTokens(text string) int {
	// –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, ~6 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
	// –£—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É
	return len(text) / 4
}

// GenerateContext –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
func (g *LLMContextGenerator) GenerateContext(ctx context.Context, projectName string, files map[string]string) (*ProjectContextLLM, error) {
	log.Printf("üß† [STEP 0] Starting LLM-based project context generation for '%s' with %d files (limit: %d tokens)", projectName, len(files), g.maxTokens)
	start := time.Now()

	context := &ProjectContextLLM{
		ProjectName: projectName,
		GeneratedAt: time.Now(),
		TotalFiles:  len(files),
		Files:       make(map[string]LLMFileContext),
		TokensLimit: g.maxTokens,
		Structure: ProjectStructure{
			Directories: make([]Directory, 0),
			FileTypes:   make([]FileType, 0),
		},
	}
	log.Printf("üß† [STEP 1] Context structure initialized (%.2fs)", time.Since(start).Seconds())

	// 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏ –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
	step1Start := time.Now()
	context.Language = g.detectMainLanguage(files)
	log.Printf("üß† [STEP 2] Main language detected: %s (%.2fs)", context.Language, time.Since(step1Start).Seconds())

	step2Start := time.Now()
	g.analyzeProjectStructure(files, &context.Structure)
	log.Printf("üß† [STEP 3] Project structure analyzed: %d dirs, %d file types (%.2fs)", len(context.Structure.Directories), len(context.Structure.FileTypes), time.Since(step2Start).Seconds())

	// 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
	step3Start := time.Now()
	log.Printf("üß† [STEP 4] Starting LLM project description generation...")
	if err := g.generateProjectDescription(ctx, context, files); err != nil {
		log.Printf("‚ö†Ô∏è [STEP 4] Failed to generate project description: %v (%.2fs)", err, time.Since(step3Start).Seconds())
		context.Description = fmt.Sprintf("%s project", projectName)
	} else {
		log.Printf("üß† [STEP 4] Project description generated: '%s' (%.2fs)", context.Description, time.Since(step3Start).Seconds())
	}

	// 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
	step4Start := time.Now()
	context.Dependencies = g.extractDependencies(files, context.Language)
	log.Printf("üß† [STEP 5] Dependencies extracted: %d deps (%.2fs)", len(context.Dependencies), time.Since(step4Start).Seconds())

	// 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
	step5Start := time.Now()
	fileList := g.sortFilesByImportance(files)
	log.Printf("üß† [STEP 6] Files sorted by importance: %d files (%.2fs)", len(fileList), time.Since(step5Start).Seconds())
	if len(fileList) > 0 {
		log.Printf("üß† [STEP 6] Top 5 files: %v", fileList[:min(5, len(fileList))])
	}

	// 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
	tokenBudget := g.maxTokens - g.tokenEstimator.EstimateTokens(context.Description) - 500 // –†–µ–∑–µ—Ä–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
	log.Printf("üß† [STEP 7] Starting individual file context generation. Token budget: %d tokens", tokenBudget)
	processedFiles := 0

	for i, filePath := range fileList {
		if tokenBudget <= 0 {
			log.Printf("‚ö†Ô∏è [STEP 7] Token budget exhausted after %d files, skipping remaining %d files", processedFiles, len(fileList)-i)
			break
		}

		fileStart := time.Now()
		log.Printf("üß† [STEP 7.%d] Processing file %d/%d: %s (budget: %d tokens)", i+1, i+1, len(fileList), filePath, tokenBudget)

		fileContent := files[filePath]
		fileBudget := tokenBudget / 4 // 1/4 –±—é–¥–∂–µ—Ç–∞ –Ω–∞ —Ñ–∞–π–ª
		if fileBudget < 50 {
			fileBudget = 50 // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç
		}

		fileContext, err := g.generateFileContext(ctx, context, filePath, fileContent, fileBudget)
		if err != nil {
			log.Printf("‚ö†Ô∏è [STEP 7.%d] Failed to generate context for %s: %v (%.2fs)", i+1, filePath, err, time.Since(fileStart).Seconds())
			continue
		}

		context.Files[filePath] = *fileContext
		tokenBudget -= fileContext.TokensUsed
		context.TokensUsed += fileContext.TokensUsed
		processedFiles++
		log.Printf("üß† [STEP 7.%d] ‚úÖ File processed: %s (%d tokens used, %d remaining, %.2fs)", i+1, filePath, fileContext.TokensUsed, tokenBudget, time.Since(fileStart).Seconds())
	}

	log.Printf("‚úÖ [FINAL] LLM context generation completed: %d/%d files processed, %d/%d tokens used (%.2fs total)",
		len(context.Files), len(files), context.TokensUsed, context.TokensLimit, time.Since(start).Seconds())
	return context, nil
}

// generateProjectDescription –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
func (g *LLMContextGenerator) generateProjectDescription(ctx context.Context, context *ProjectContextLLM, files map[string]string) error {
	start := time.Now()
	log.Printf("üß† [DESC] Starting project description generation...")

	// –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
	var fileList []string
	for path := range files {
		fileList = append(fileList, path)
	}
	sort.Strings(fileList)
	log.Printf("üß† [DESC] File list prepared: %d files (%.2fs)", len(fileList), time.Since(start).Seconds())

	systemPrompt := `You are a code analysis assistant. Analyze the project structure and generate a concise description.

Your task is to:
1. Understand the project's main purpose based on file structure and names
2. Identify the primary technology/framework being used
3. Generate a concise description (max 100 characters)

Respond with JSON:
{
  "description": "Brief project description",
  "language": "primary programming language"
}`

	userPrompt := fmt.Sprintf(`Project: %s
Language: %s
Total files: %d

File structure:
%s

Please analyze this project structure and provide a concise description.`,
		context.ProjectName,
		context.Language,
		len(fileList),
		strings.Join(fileList[:min(20, len(fileList))], "\n")) // –ü–µ—Ä–≤—ã–µ 20 —Ñ–∞–π–ª–æ–≤

	messages := []llm.Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userPrompt},
	}
	log.Printf("üß† [DESC] Prompt prepared, sending to LLM... (%.2fs)", time.Since(start).Seconds())

	llmStart := time.Now()
	response, err := g.llmClient.Generate(ctx, messages)
	if err != nil {
		log.Printf("‚ùå [DESC] LLM request failed after %.2fs: %v", time.Since(llmStart).Seconds(), err)
		return fmt.Errorf("LLM request failed: %w", err)
	}
	log.Printf("üß† [DESC] LLM response received (%.2fs), parsing...", time.Since(llmStart).Seconds())

	var result struct {
		Description string `json:"description"`
		Language    string `json:"language"`
	}

	if err := json.Unmarshal([]byte(response.Content), &result); err != nil {
		log.Printf("‚ö†Ô∏è [DESC] JSON parsing failed, using raw response: %s", response.Content[:min(100, len(response.Content))])
		// –ï—Å–ª–∏ JSON parsing –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º raw –æ—Ç–≤–µ—Ç
		context.Description = strings.TrimSpace(response.Content)
		if len(context.Description) > 100 {
			context.Description = context.Description[:100] + "..."
		}
		log.Printf("üß† [DESC] Description set from raw response: '%s' (%.2fs total)", context.Description, time.Since(start).Seconds())
		return nil
	}

	context.Description = result.Description
	if result.Language != "" && result.Language != "unknown" {
		context.Language = result.Language
	}
	log.Printf("üß† [DESC] ‚úÖ Description parsed successfully: '%s' (%.2fs total)", context.Description, time.Since(start).Seconds())

	return nil
}

// generateFileContext –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
func (g *LLMContextGenerator) generateFileContext(ctx context.Context, projectContext *ProjectContextLLM, filePath, fileContent string, tokenBudget int) (*LLMFileContext, error) {
	start := time.Now()
	log.Printf("üß† [FILE] Starting context generation for %s (%d chars, budget: %d tokens)", filePath, len(fileContent), tokenBudget)

	fileContext := &LLMFileContext{
		Path:         filePath,
		Type:         g.getFileType(filepath.Ext(filePath)),
		Size:         len(fileContent),
		LastModified: time.Now(),
		NeedsUpdate:  false,
	}

	// –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (< 200 —Å–∏–º–≤–æ–ª–æ–≤) —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
	if len(fileContent) < 200 {
		fileContext.Summary = fmt.Sprintf("Small %s file (%d chars)", fileContext.Type, len(fileContent))
		fileContext.TokensUsed = g.tokenEstimator.EstimateTokens(fileContext.Summary)
		log.Printf("üß† [FILE] ‚úÖ Small file processed: %s (%.2fs)", filePath, time.Since(start).Seconds())
		return fileContext, nil
	}

	// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LLM —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–¥–∞
	if !g.isCodeFile(fileContext.Type) {
		fileContext.Summary = g.generateGenericSummary(fileContent, fileContext.Type)
		fileContext.TokensUsed = g.tokenEstimator.EstimateTokens(fileContext.Summary)
		log.Printf("üß† [FILE] ‚úÖ Non-code file processed: %s (%.2fs)", filePath, time.Since(start).Seconds())
		return fileContext, nil
	}

	log.Printf("üß† [FILE] Preparing LLM request for code file: %s (%.2fs)", filePath, time.Since(start).Seconds())

	// –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
	request := ContextGenerationRequest{
		ProjectName: projectContext.ProjectName,
		Language:    projectContext.Language,
		FilePath:    filePath,
		TokenBudget: tokenBudget,
	}

	// –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞, LLM –∑–∞–ø—Ä–æ—Å–∏—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–µ—Ä–µ–∑ MCP
	if len(fileContent) > 2000 {
		request.FileContent = "" // LLM –¥–æ–ª–∂–µ–Ω –∑–∞–ø—Ä–æ—Å–∏—Ç—å —á–µ—Ä–µ–∑ MCP
		log.Printf("üß† [FILE] Large file (%d chars), content will be requested via MCP", len(fileContent))
	} else {
		request.FileContent = fileContent
		log.Printf("üß† [FILE] Sending file content directly (%d chars)", len(fileContent))
	}

	// –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
	for path := range projectContext.Files {
		request.FileList = append(request.FileList, path)
	}

	systemPrompt := g.buildFileAnalysisSystemPrompt()
	userPrompt := g.buildFileAnalysisUserPrompt(request)
	log.Printf("üß† [FILE] Prompts prepared, sending to LLM... (%.2fs)", time.Since(start).Seconds())

	messages := []llm.Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userPrompt},
	}

	llmStart := time.Now()
	response, err := g.llmClient.Generate(ctx, messages)
	if err != nil {
		log.Printf("‚ùå [FILE] LLM analysis failed for %s after %.2fs: %v", filePath, time.Since(llmStart).Seconds(), err)
		return nil, fmt.Errorf("LLM analysis failed: %w", err)
	}
	log.Printf("üß† [FILE] LLM response received for %s (%.2fs), parsing...", filePath, time.Since(llmStart).Seconds())

	// –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç LLM
	var llmResponse ContextGenerationResponse
	if err := json.Unmarshal([]byte(response.Content), &llmResponse); err != nil {
		log.Printf("‚ö†Ô∏è [FILE] JSON parsing failed for %s, using raw response: %s", filePath, response.Content[:min(100, len(response.Content))])
		// Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º raw –æ—Ç–≤–µ—Ç –∫–∞–∫ summary
		fileContext.Summary = strings.TrimSpace(response.Content)
		if len(fileContext.Summary) > 300 {
			fileContext.Summary = fileContext.Summary[:300] + "..."
		}
	} else {
		log.Printf("üß† [FILE] JSON parsed successfully for %s: %d key elements, %d deps", filePath, len(llmResponse.KeyElements), len(llmResponse.Dependencies))
		fileContext.Summary = llmResponse.Summary
		fileContext.KeyElements = llmResponse.KeyElements
		fileContext.Purpose = llmResponse.Purpose
		fileContext.Dependencies = llmResponse.Dependencies
	}

	// –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
	fileContext.TokensUsed = g.tokenEstimator.EstimateTokens(fileContext.Summary + strings.Join(fileContext.KeyElements, " ") + fileContext.Purpose)
	log.Printf("üß† [FILE] ‚úÖ Context generated for %s: %d tokens used (%.2fs total)", filePath, fileContext.TokensUsed, time.Since(start).Seconds())

	return fileContext, nil
}

// buildFileAnalysisSystemPrompt —Å–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
func (g *LLMContextGenerator) buildFileAnalysisSystemPrompt() string {
	return `You are a code analysis assistant. Your task is to analyze source code files and generate concise, structured descriptions.

For each file, you should:
1. Understand the file's purpose within the project
2. Identify key elements (functions, classes, interfaces, etc.)
3. Determine dependencies on other files
4. Generate a concise summary

IMPORTANT GUIDELINES:
- Keep descriptions concise but informative
- Focus on the file's role in the project architecture
- Identify public/exported APIs and main functionality
- If file content is not provided, you can use MCP tools to request it
- Stay within token budget provided in the request

Respond with JSON:
{
  "summary": "Brief description of file's purpose and content (max 200 chars)",
  "key_elements": ["function1", "class1", "interface1"], 
  "purpose": "Role of this file in the project (max 100 chars)",
  "dependencies": ["file1.go", "file2.go"]
}`
}

// buildFileAnalysisUserPrompt —Å–æ–∑–¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
func (g *LLMContextGenerator) buildFileAnalysisUserPrompt(request ContextGenerationRequest) string {
	prompt := fmt.Sprintf(`Project: %s (%s)
File: %s
Token budget: %d tokens

`, request.ProjectName, request.Language, request.FilePath, request.TokenBudget)

	if request.FileContent != "" {
		prompt += fmt.Sprintf("File content:\n```\n%s\n```\n\n", request.FileContent)
	} else {
		prompt += "File content: (large file - use MCP tools to read if needed)\n\n"
	}

	if len(request.FileList) > 0 {
		prompt += fmt.Sprintf("Project files for context:\n%s\n\n", strings.Join(request.FileList[:min(10, len(request.FileList))], "\n"))
	}

	prompt += "Please analyze this file and provide a structured description following the JSON format."
	return prompt
}

// UpdateFileContext –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
func (g *LLMContextGenerator) UpdateFileContext(ctx context.Context, projectContext *ProjectContextLLM, filePath, newContent string) error {
	log.Printf("üîÑ Updating LLM context for file: %s", filePath)

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ä–∞–∑–º–µ—Ä —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ)
	if existingContext, exists := projectContext.Files[filePath]; exists {
		sizeDiff := abs(len(newContent) - existingContext.Size)
		if sizeDiff < len(newContent)/10 { // –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–µ–Ω—å—à–µ 10%
			log.Printf("‚è≠Ô∏è Skipping update for %s - minimal changes", filePath)
			return nil
		}
	}

	// –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ç–æ–∫–µ–Ω –±—é–¥–∂–µ—Ç
	tokenBudget := (projectContext.TokensLimit - projectContext.TokensUsed) / 2 // –ü–æ–ª–æ–≤–∏–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞

	// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–∞–π–ª–∞
	newFileContext, err := g.generateFileContext(ctx, projectContext, filePath, newContent, tokenBudget)
	if err != nil {
		return fmt.Errorf("failed to update file context: %w", err)
	}

	// –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
	if oldContext, exists := projectContext.Files[filePath]; exists {
		projectContext.TokensUsed -= oldContext.TokensUsed // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã
	}

	projectContext.Files[filePath] = *newFileContext
	projectContext.TokensUsed += newFileContext.TokensUsed

	log.Printf("‚úÖ Updated context for %s: %d tokens", filePath, newFileContext.TokensUsed)
	return nil
}

// RemoveFileContext —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
func (g *LLMContextGenerator) RemoveFileContext(projectContext *ProjectContextLLM, filePath string) {
	if existingContext, exists := projectContext.Files[filePath]; exists {
		projectContext.TokensUsed -= existingContext.TokensUsed
		delete(projectContext.Files, filePath)
		projectContext.TotalFiles--
		log.Printf("üóëÔ∏è Removed context for %s", filePath)
	}
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã

func (g *LLMContextGenerator) detectMainLanguage(files map[string]string) string {
	langCounts := make(map[string]int)

	for path := range files {
		ext := strings.ToLower(filepath.Ext(path))
		switch ext {
		case ".go":
			langCounts["go"]++
		case ".js", ".ts":
			langCounts["javascript"]++
		case ".py":
			langCounts["python"]++
		case ".java":
			langCounts["java"]++
		case ".cpp", ".cc", ".cxx":
			langCounts["cpp"]++
		case ".c":
			langCounts["c"]++
		case ".rs":
			langCounts["rust"]++
		}
	}

	maxCount := 0
	mainLang := "unknown"
	for lang, count := range langCounts {
		if count > maxCount {
			maxCount = count
			mainLang = lang
		}
	}

	return mainLang
}

func (g *LLMContextGenerator) analyzeProjectStructure(files map[string]string, structure *ProjectStructure) {
	dirCounts := make(map[string]int)
	fileTypeCounts := make(map[string]int)
	langMap := make(map[string]string)

	for path := range files {
		dir := filepath.Dir(path)
		if dir == "." {
			dir = "root"
		}
		dirCounts[dir]++

		ext := strings.ToLower(filepath.Ext(path))
		fileTypeCounts[ext]++

		switch ext {
		case ".go":
			langMap[ext] = "Go"
		case ".js":
			langMap[ext] = "JavaScript"
		case ".ts":
			langMap[ext] = "TypeScript"
		case ".py":
			langMap[ext] = "Python"
		case ".java":
			langMap[ext] = "Java"
		case ".md":
			langMap[ext] = "Markdown"
		case ".json":
			langMap[ext] = "JSON"
		default:
			langMap[ext] = "Other"
		}
	}

	for dir, count := range dirCounts {
		structure.Directories = append(structure.Directories, Directory{
			Path:      dir,
			FileCount: count,
			Purpose:   g.guessDirPurpose(dir),
		})
	}

	for ext, count := range fileTypeCounts {
		if ext != "" {
			structure.FileTypes = append(structure.FileTypes, FileType{
				Extension: ext,
				Count:     count,
				Language:  langMap[ext],
			})
		}
	}
}

func (g *LLMContextGenerator) guessDirPurpose(dir string) string {
	lowerDir := strings.ToLower(dir)

	switch {
	case strings.Contains(lowerDir, "cmd") || strings.Contains(lowerDir, "main"):
		return "Application entry points"
	case strings.Contains(lowerDir, "internal"):
		return "Internal application code"
	case strings.Contains(lowerDir, "pkg"):
		return "Library code"
	case strings.Contains(lowerDir, "api"):
		return "API definitions and handlers"
	case strings.Contains(lowerDir, "web"):
		return "Web interface"
	case strings.Contains(lowerDir, "test"):
		return "Test files"
	case dir == "root":
		return "Project root files"
	default:
		return "Source code"
	}
}

func (g *LLMContextGenerator) extractDependencies(files map[string]string, language string) []string {
	if language == "go" {
		return g.extractGoDependencies(files)
	}
	return nil
}

func (g *LLMContextGenerator) extractGoDependencies(files map[string]string) []string {
	if goMod, exists := files["go.mod"]; exists {
		return g.parseGoModDependencies(goMod)
	}
	return nil
}

func (g *LLMContextGenerator) parseGoModDependencies(goMod string) []string {
	var deps []string
	lines := strings.Split(goMod, "\n")
	inRequire := false

	for _, line := range lines {
		line = strings.TrimSpace(line)

		if strings.HasPrefix(line, "require (") {
			inRequire = true
			continue
		}

		if inRequire && line == ")" {
			break
		}

		if inRequire && line != "" && !strings.HasPrefix(line, "//") {
			parts := strings.Fields(line)
			if len(parts) >= 2 {
				deps = append(deps, parts[0])
			}
		}
	}

	sort.Strings(deps)
	return deps
}

func (g *LLMContextGenerator) sortFilesByImportance(files map[string]string) []string {
	type fileInfo struct {
		path  string
		score int
	}

	var fileList []fileInfo
	for path := range files {
		score := g.calculateFileImportance(path, files[path])
		fileList = append(fileList, fileInfo{path: path, score: score})
	}

	sort.Slice(fileList, func(i, j int) bool {
		return fileList[i].score > fileList[j].score
	})

	result := make([]string, len(fileList))
	for i, fi := range fileList {
		result[i] = fi.path
	}
	return result
}

func (g *LLMContextGenerator) calculateFileImportance(path, content string) int {
	score := 0
	lowerPath := strings.ToLower(path)

	// main.go —Ñ–∞–π–ª—ã - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
	if strings.Contains(lowerPath, "main.go") {
		score += 1000
	}

	// –§–∞–π–ª—ã —Å main —Ñ—É–Ω–∫—Ü–∏–µ–π
	if strings.Contains(content, "func main(") {
		score += 500
	}

	// API –∏ web —Ñ–∞–π–ª—ã –≤–∞–∂–Ω—ã
	if strings.Contains(lowerPath, "api") ||
		strings.Contains(lowerPath, "web") ||
		strings.Contains(lowerPath, "handler") {
		score += 100
	}

	// .go —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
	if strings.HasSuffix(lowerPath, ".go") {
		score += 50
	}

	return score
}

func (g *LLMContextGenerator) getFileType(ext string) string {
	switch strings.ToLower(ext) {
	case ".go":
		return "go"
	case ".js":
		return "javascript"
	case ".ts":
		return "typescript"
	case ".py":
		return "python"
	case ".java":
		return "java"
	case ".md":
		return "markdown"
	case ".json":
		return "json"
	default:
		return "other"
	}
}

func (g *LLMContextGenerator) isCodeFile(fileType string) bool {
	codeTypes := []string{"go", "javascript", "typescript", "python", "java", "cpp", "c", "rust"}
	for _, ct := range codeTypes {
		if fileType == ct {
			return true
		}
	}
	return false
}

func (g *LLMContextGenerator) generateGenericSummary(content, fileType string) string {
	lines := len(strings.Split(content, "\n"))

	switch fileType {
	case "markdown":
		if strings.HasPrefix(content, "# ") {
			firstLine := strings.Split(content, "\n")[0]
			return strings.TrimPrefix(firstLine, "# ")
		}
		return fmt.Sprintf("Markdown documentation (%d lines)", lines)
	case "json":
		if strings.Contains(content, `"dependencies"`) {
			return "Package dependencies configuration"
		}
		if strings.Contains(content, `"scripts"`) {
			return "Build scripts configuration"
		}
		return "JSON configuration file"
	default:
		return fmt.Sprintf("%s file (%d lines)", fileType, lines)
	}
}

// GenerateContextWithPreloadedFiles –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É—è —É–∂–µ –≥–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
func (g *LLMContextGenerator) GenerateContextWithPreloadedFiles(ctx context.Context, projectName string, fileList []string, fileContentMap map[string]string) (*ProjectContextLLM, error) {
	log.Printf("üß† [PARALLEL] Starting optimized LLM context generation for '%s' with %d preloaded files (limit: %d tokens)", projectName, len(fileList), g.maxTokens)
	start := time.Now()

	context := &ProjectContextLLM{
		ProjectName: projectName,
		GeneratedAt: time.Now(),
		TotalFiles:  len(fileList),
		Files:       make(map[string]LLMFileContext),
		TokensLimit: g.maxTokens,
		Structure: ProjectStructure{
			Directories: make([]Directory, 0),
			FileTypes:   make([]FileType, 0),
		},
	}
	log.Printf("üß† [PARALLEL] Context structure initialized (%.2fs)", time.Since(start).Seconds())

	// 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —Ñ–∞–π–ª–∞–º
	step1Start := time.Now()
	context.Language = g.detectMainLanguageFromList(fileList)
	log.Printf("üß† [PARALLEL] Main language detected: %s (%.2fs)", context.Language, time.Since(step1Start).Seconds())

	// 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
	step2Start := time.Now()
	g.analyzeProjectStructureFromList(fileList, &context.Structure)
	log.Printf("üß† [PARALLEL] Project structure analyzed: %d dirs, %d file types (%.2fs)", len(context.Structure.Directories), len(context.Structure.FileTypes), time.Since(step2Start).Seconds())

	// 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
	step3Start := time.Now()
	log.Printf("üß† [PARALLEL] Starting LLM project description generation...")
	if err := g.generateProjectDescriptionFromList(ctx, context, fileList); err != nil {
		log.Printf("‚ö†Ô∏è [PARALLEL] Failed to generate project description: %v (%.2fs)", err, time.Since(step3Start).Seconds())
		context.Description = fmt.Sprintf("%s project", projectName)
	} else {
		log.Printf("üß† [PARALLEL] Project description generated: '%s' (%.2fs)", context.Description, time.Since(step3Start).Seconds())
	}

	// 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
	step4Start := time.Now()
	context.Dependencies = g.extractDependenciesFromMap(fileContentMap, context.Language)
	log.Printf("üß† [PARALLEL] Dependencies extracted: %d deps (%.2fs)", len(context.Dependencies), time.Since(step4Start).Seconds())

	// 5. –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
	step5Start := time.Now()
	sortedFiles := g.sortFilesByImportanceFromList(fileList, fileContentMap)
	log.Printf("üß† [PARALLEL] Files sorted by importance: %d files (%.2fs)", len(sortedFiles), time.Since(step5Start).Seconds())
	if len(sortedFiles) > 0 {
		log.Printf("üß† [PARALLEL] Top 5 files: %v", sortedFiles[:min(5, len(sortedFiles))])
	}

	// 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
	tokenBudget := g.maxTokens - g.tokenEstimator.EstimateTokens(context.Description) - 500 // –†–µ–∑–µ—Ä–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
	log.Printf("üß† [PARALLEL] Starting individual file context generation. Token budget: %d tokens", tokenBudget)
	processedFiles := 0

	for i, filePath := range sortedFiles {
		if tokenBudget <= 0 {
			log.Printf("‚ö†Ô∏è [PARALLEL] Token budget exhausted after %d files, skipping remaining %d files", processedFiles, len(sortedFiles)-i)
			break
		}

		fileStart := time.Now()
		log.Printf("üß† [PARALLEL] Processing file %d/%d: %s (budget: %d tokens)", i+1, len(sortedFiles), filePath, tokenBudget)

		// –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç—ã
		fileContent, exists := fileContentMap[filePath]
		if !exists {
			log.Printf("‚ö†Ô∏è [PARALLEL] File content not found in map: %s, skipping", filePath)
			continue
		}

		fileBudget := tokenBudget / 4 // 1/4 –±—é–¥–∂–µ—Ç–∞ –Ω–∞ —Ñ–∞–π–ª
		if fileBudget < 50 {
			fileBudget = 50 // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç
		}

		fileContext, err := g.generateFileContext(ctx, context, filePath, fileContent, fileBudget)
		if err != nil {
			log.Printf("‚ö†Ô∏è [PARALLEL] Failed to generate context for %s: %v (%.2fs)", filePath, err, time.Since(fileStart).Seconds())
			continue
		}

		context.Files[filePath] = *fileContext
		tokenBudget -= fileContext.TokensUsed
		context.TokensUsed += fileContext.TokensUsed
		processedFiles++
		log.Printf("üß† [PARALLEL] ‚úÖ File processed: %s (%d tokens used, %d remaining, %.2fs)", filePath, fileContext.TokensUsed, tokenBudget, time.Since(fileStart).Seconds())
	}

	log.Printf("‚úÖ [PARALLEL] LLM context generation completed: %d/%d files processed, %d/%d tokens used (%.2fs total)",
		len(context.Files), len(fileList), context.TokensUsed, context.TokensLimit, time.Since(start).Seconds())
	return context, nil
}

// generateProjectDescriptionFromList –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
func (g *LLMContextGenerator) generateProjectDescriptionFromList(ctx context.Context, context *ProjectContextLLM, fileList []string) error {
	start := time.Now()
	log.Printf("üß† [DESC-LIST] Starting project description generation from file list...")

	systemPrompt := `You are a code analysis assistant. Analyze the project structure and generate a concise description.

Your task is to:
1. Understand the project's main purpose based on file structure and names
2. Identify the primary technology/framework being used
3. Generate a concise description (max 100 characters)

Respond with JSON:
{
  "description": "Brief project description",
  "language": "primary programming language"
}`

	userPrompt := fmt.Sprintf(`Project: %s
Language: %s
Total files: %d

File structure:
%s

Please analyze this project structure and provide a concise description.`,
		context.ProjectName,
		context.Language,
		len(fileList),
		strings.Join(fileList[:min(20, len(fileList))], "\n")) // –ü–µ—Ä–≤—ã–µ 20 —Ñ–∞–π–ª–æ–≤

	messages := []llm.Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userPrompt},
	}
	log.Printf("üß† [DESC-LIST] Prompt prepared, sending to LLM... (%.2fs)", time.Since(start).Seconds())

	llmStart := time.Now()
	response, err := g.llmClient.Generate(ctx, messages)
	if err != nil {
		log.Printf("‚ùå [DESC-LIST] LLM request failed after %.2fs: %v", time.Since(llmStart).Seconds(), err)
		return fmt.Errorf("LLM request failed: %w", err)
	}
	log.Printf("üß† [DESC-LIST] LLM response received (%.2fs), parsing...", time.Since(llmStart).Seconds())

	var result struct {
		Description string `json:"description"`
		Language    string `json:"language"`
	}

	if err := json.Unmarshal([]byte(response.Content), &result); err != nil {
		log.Printf("‚ö†Ô∏è [DESC-LIST] JSON parsing failed, using raw response: %s", response.Content[:min(100, len(response.Content))])
		context.Description = strings.TrimSpace(response.Content)
		if len(context.Description) > 100 {
			context.Description = context.Description[:100] + "..."
		}
		log.Printf("üß† [DESC-LIST] Description set from raw response: '%s' (%.2fs total)", context.Description, time.Since(start).Seconds())
		return nil
	}

	context.Description = result.Description
	if result.Language != "" && result.Language != "unknown" {
		context.Language = result.Language
	}
	log.Printf("üß† [DESC-LIST] ‚úÖ Description parsed successfully: '%s' (%.2fs total)", context.Description, time.Since(start).Seconds())

	return nil
}

// detectMainLanguageFromList –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ –ø–æ —Å–ø–∏—Å–∫—É —Ñ–∞–π–ª–æ–≤
func (g *LLMContextGenerator) detectMainLanguageFromList(fileList []string) string {
	langCounts := make(map[string]int)

	for _, path := range fileList {
		ext := strings.ToLower(filepath.Ext(path))
		switch ext {
		case ".go":
			langCounts["go"]++
		case ".js", ".ts":
			langCounts["javascript"]++
		case ".py":
			langCounts["python"]++
		case ".java":
			langCounts["java"]++
		case ".cpp", ".cc", ".cxx":
			langCounts["cpp"]++
		case ".c":
			langCounts["c"]++
		case ".rs":
			langCounts["rust"]++
		}
	}

	maxCount := 0
	mainLang := "unknown"
	for lang, count := range langCounts {
		if count > maxCount {
			maxCount = count
			mainLang = lang
		}
	}

	return mainLang
}

// analyzeProjectStructureFromList –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —Å–ø–∏—Å–∫—É —Ñ–∞–π–ª–æ–≤
func (g *LLMContextGenerator) analyzeProjectStructureFromList(fileList []string, structure *ProjectStructure) {
	dirCounts := make(map[string]int)
	fileTypeCounts := make(map[string]int)
	langMap := make(map[string]string)

	for _, path := range fileList {
		dir := filepath.Dir(path)
		if dir == "." {
			dir = "root"
		}
		dirCounts[dir]++

		ext := strings.ToLower(filepath.Ext(path))
		fileTypeCounts[ext]++

		switch ext {
		case ".go":
			langMap[ext] = "Go"
		case ".js":
			langMap[ext] = "JavaScript"
		case ".ts":
			langMap[ext] = "TypeScript"
		case ".py":
			langMap[ext] = "Python"
		case ".java":
			langMap[ext] = "Java"
		case ".md":
			langMap[ext] = "Markdown"
		case ".json":
			langMap[ext] = "JSON"
		default:
			langMap[ext] = "Other"
		}
	}

	for dir, count := range dirCounts {
		structure.Directories = append(structure.Directories, Directory{
			Path:      dir,
			FileCount: count,
			Purpose:   g.guessDirPurpose(dir),
		})
	}

	for ext, count := range fileTypeCounts {
		if ext != "" {
			structure.FileTypes = append(structure.FileTypes, FileType{
				Extension: ext,
				Count:     count,
				Language:  langMap[ext],
			})
		}
	}
}

// extractDependenciesFromMap –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ –∫–∞—Ä—Ç—ã —Ñ–∞–π–ª–æ–≤
func (g *LLMContextGenerator) extractDependenciesFromMap(fileMap map[string]string, language string) []string {
	if language == "go" {
		if goMod, exists := fileMap["go.mod"]; exists {
			return g.parseGoModDependencies(goMod)
		}
	}
	return nil
}

// sortFilesByImportanceFromList —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Å–ø–∏—Å–æ–∫ –∏ –∫–∞—Ä—Ç—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
func (g *LLMContextGenerator) sortFilesByImportanceFromList(fileList []string, fileContentMap map[string]string) []string {
	type fileInfo struct {
		path  string
		score int
	}

	var files []fileInfo
	for _, path := range fileList {
		content := fileContentMap[path] // –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
		score := g.calculateFileImportance(path, content)
		files = append(files, fileInfo{path: path, score: score})
	}

	sort.Slice(files, func(i, j int) bool {
		return files[i].score > files[j].score
	})

	result := make([]string, len(files))
	for i, fi := range files {
		result[i] = fi.path
	}
	return result
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func abs(x int) int {
	if x < 0 {
		return -x
	}
	return x
}
